# 决策树

## 1. 决策树的基本概念

> 决策树最大的优势是处理离散变量

分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成。内部节点表示一个特征或属性，叶节点表示一个类。

决策树学习采用的是**自顶向下**的递归方法, 其基本思想是以**信息熵**为度量构造一棵**熵值 下降最快**的树,到叶子节点处的熵值为零, 此时每个叶节点中的实例都属于同一类。

最大优点: 可以自学习。在学习的过程中,不需要使用者了解过多背景知识,只需要对训练实例进行较好的标注,就能够进行学习。显然,属于**有监督学习**。

决策树学习算法通常采用**启发式方法**，包含<mark style="color:orange;">**特征选择，决策树的生成，决策树的剪枝**</mark>过程。

### 信息增益

*   熵：表示随机变量不确定性的度量。熵越大，随机变量的不确定性就越大。

    $$
    H(p)=-\sum\limits_{i=1}^n p_i\log p_i
    $$
*   条件熵：表示在已知随机变量X的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望。

    $$
    H(Y|X)=\sum\limits_{i=1}^np_iH(Y|X=x_i)
    $$
*   信息增益：特征A对训练数据集D的信息增益$$g(D,A)$$，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差。

    $$
    g(D,A)=H(D)-H(D|A)
    $$

    其中$$C_k$$是D中属于第k类的样本子集，K是类的个数，$$\sum\limits_{k=1}^K|C_k|=|D|$$；根据特征A的取值将D划分为n个子集，$$\sum\limits_{i=1}^n |D_i|=|D|$$；记子集$$D_i$$中属于类$$C_k$$的样本的集合为$$D_{ik}$$

    $$
    H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
    $$

    $$
    H(D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}
    $$

    > 一般地，熵$$H(Y)$$与条件熵$$H(Y|X)$$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。
*   信息增益比：特征A对训练数据集D的信息增益比$$g_R(D,A)$$定义为其信息增益$$g(D,A)$$与训练数据集D关于特征A的值的熵$$H_A(D)$$之比。

    $$
    g_R(D,A)=\dfrac{g(D,A)}{H_A(D)}
    $$

    其中

    $$
    H_A(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2}\left(\frac{\left|D_{i}\right|}{|D|}\right),\quad n\text{是特征}A\text{取值的个数}
    $$

### 三种生成算法

1. ID3 --- **信息增益** **最大**的准则
2. C4.5 --- **信息增益比** **最大**的准则
3. CART
   * 回归树: **平方误差** **最小** 的准则
   * 分类树: **基尼指数** **最小**的准则

## 2. ID3算法

ID3算法核心是在决策树各个节点上应用<mark style="color:blue;">信息增益准则选择特征</mark>，递归地构造决策树。根据信息增益准则的特征选择方法是：对训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

从根节点开始，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点，再对子节点递归地调用以上方法，构造决策树。

<mark style="color:purple;">ID3相当于用极大似然法进行概率模型的选择</mark>。

ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。

## 3. C4.5算法

C4.5算法继承了ID3算法的优点，并在以下几方面对ID3算法进行了改进：

* <mark style="color:blue;">用信息增益比来选择特征</mark>，克服了用信息增益选择特征时偏向选择取值较多的特征的不足
* 在树构造过程中进行剪枝
* 能够完成对连续属性的离散化处理
* 能够对不完整数据进行处理

## 4. CART算法

分类与回归树（classification and regression tree, CART），既可以用于分类，也可以用于回归。以下将用于分类与回归的树统称为决策树。

> CART算法与ID3和C4.5的区别
>
> * 可以做回归
> * 分类中用基尼指数最小来选择特征
> * 只能是二叉树

CART算法由以下两步组成：

*   #### 决策树生成

    基于训练数据集生成决策树，生成的决策树要尽量大

    <mark style="color:blue;">决策树的生成就是递归地构建二叉树的过程</mark>。对**回归树用平方误差最小化准则**，对**分类树用基尼指数( Gini index)最小化准则**，进行特征选择，生成二叉树。

    样本集合D的基尼指数：

    $$
    \operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}
    $$

    其中$$C_k$$是D中属于第k类的样本子集，K是类的个数。

    特征A条件下集合D的基尼指数：

    $$
    \operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
    $$
*   #### 决策树剪枝

    由于生成的决策树存在过拟合的问题，需要对它进行剪枝。

    用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准

## 5. 随机森林

随机森林是以决策树为基本分类器的一个集成学习模型，它包含多个由Bagging集成学习技术训练得到的决策树。

![](https://gitee.com/liuyh9909/note-imgs/raw/master/img/20220102141211.png)

PAC$$\rightarrow$$Bootstrap$$\rightarrow$$Bagging$$\rightarrow$$Random Forest$$\leftarrow$$CART

* Random：
  * 对训练样本的随机抽取
  * 对特征的随机抽取
    * RI（随机输入选择）
    * RC（随机线性组合）
* 核心：<mark style="color:red;">由弱变强</mark>。每棵决策树由于只用了部分特征、部分样本训练而成，可能单个的分类准确率并不是很高。但是当一群这样的决策树组合起来分别对输入数据作出判断时，可以带来较高的准确率。
* 参数：
  * 树节点预选的变量个数（随机挑多少个特征）
  * 随机森林中树的个数
