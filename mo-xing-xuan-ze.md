# 模型选择

## 1. 引言

![](https://gitee.com/liuyh9909/note-imgs/raw/master/img/20220102141947.png)

## 2. 模型选择原则

* #### 没有免费的午餐定理（No Free Lunch, NFL)
  * 对于整个函数集(类)而言，不存在万能的最佳算法
* #### 丑小鸭定理（Ugly Ducking)
  * 不存在与问题无关的最优的特征/属性集合
  * 不存在与问题无关的模式“相似性度量”
* #### Occam剃刀原理（Occam’s Razor）
  * 如无必要，勿增实体 ——即简单有效原理
  * 剔除所有累赘 ——简约而不简单
* #### 最小描述长度原理（Minimum Description Length, MDL）
  * 应该选择尽可能简单的分类器或模型

## 3. 模型评价标准

* #### 统计检验方法
* #### 偏差和方差
* #### 操作接收者特征曲线 ROC (对两类分类问题)
* #### 模型的复杂度
* #### 训练数据的多少对泛化性能的影响
* #### 分类器训练的评价过程
* #### 训练样本的划分
  * **交叉验证：模型参数选择**

## 4. 分类器集成

* #### 集成学习常用技术手段
  *   **通过处理训练数据（bagging, boosting)**

      对训练样本进行随机分组，对错分样本进行加权。
  *   **通过处理特征**

      每次只选择一部分特征来训练分类器
  *   **通过处理类别标号**

      对多类问题，一对一策略、一对多策略。
  *   **通过改进学习方法**

      变更学习参数(如多核学习)、模型结构(如神经网络结构) 等
* #### 分类器集成算法分类
  * **Bagging**
    * 训练一组基分类器，每个基分类器通过一个 bootstrap训练样本集来训练
    * 一个bootstrap训练样本集是通过有放回地随机从一个给定的数据中抽样得到
  * **Random subspace （随机子空间）**
    * 随机子空间通常也被称为属性装袋 (attribute bagging)
    * 随机子空间的基分类器通常由线性分类器、支持向量机等组成
  * **Boosting / Adaboost**
    * 在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能
  * **Random Forest（随机森林）**
    * 见\[\[决策树]]

## 5. Adaboost算法

### 算法思想

提升方法是将弱学习算法提升为强学习算法的统计学习方法。在分类学习中，提升方法通过反复修改训练数据的权值分布，构建一系列基本分类器（弱分类器），并将这些分类器线性组合，构成一个强分类器。

对提升方法来说，需要回答两个问题：

*   **在每轮训练中，如何改变训练数据的权值或分布？**

    Adaboost的做法是：提高那些被前一轮弱分类器分错的样本的权重，降低已经被正确分类的样本的权重。错分的样本将在下一轮弱分类器中得到更多关注。于是分类问题被一系列弱分类器“分而治之”
*   **如何将一系列的弱分类器组合成一个强分类器？**

    关于弱分类器的组合，Adaboost的做法是：采用加权(多数)表决的方法。具体地，加大分类错误率较小的弱分类器的权重，使其在表决中起更大的作用

Adaboost的巧妙之处在于将这些想法融合于一个算法之中！

<mark style="color:blue;">AdaBoost算法的特点是通过迭代每次学习一个基本分类器。每次迭代中，提高那些被前一轮分类器错误分类数据的权值，而降低那些被正确分类的数据的权值。最后，AdaBoost将基本分类器的线性组合作为强分类器，其中给分类误差率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值</mark>。

### Adaboost算法步骤——以二分类为例

![](https://gitee.com/liuyh9909/note-imgs/raw/master/img/20220102153755.png)

![](https://gitee.com/liuyh9909/note-imgs/raw/master/img/20220102153904.png)

![](https://gitee.com/liuyh9909/note-imgs/raw/master/img/20220102154010.png)

### Reference

* \[1] [AdaBoost](https://blog.paperspace.com/adaboost-optimizer/)
